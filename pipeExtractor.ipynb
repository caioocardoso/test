{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590150d5",
   "metadata": {},
   "source": [
    "# Dependências"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d6477",
   "metadata": {},
   "source": [
    "# 1.Extração de Texto de PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed9f00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_pymupdf(pdf_path):\n",
    "  text = \"\"\n",
    "  try:\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "      for page in range(len(doc)):\n",
    "        page_text = doc.load_page(page)\n",
    "        text += page_text.get_text(\"text\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error reading {pdf_path} with PyMuPDF: {e}\")\n",
    "    return None\n",
    "  return text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0635ac",
   "metadata": {},
   "source": [
    "# 2.Pré-processamento do Texto Extraído"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae47c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: spaCy model loaded.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "try:\n",
    "  nlp = spacy.load(\"pt_core_news_lg\")\n",
    "  nlp.max_length = 3000000\n",
    "  print(\"INFO: spaCy model loaded.\")\n",
    "except OSError:\n",
    "  print(\"ERROR: 'pt_core_news_lg' not found. Trying 'pt_core_news_md.\")\n",
    "  try:\n",
    "    nlp = spacy.load(\"pt_core_news_md\")\n",
    "    print(\"INFO: spaCy model loaded.\")\n",
    "    print(\"INFO: 'pt_core_news_md' model is smaller and may not perform as well as 'pt_core_news_lg'.\")\n",
    "  except OSError:\n",
    "    print(\"ERROR: 'pt_core_news_md' not found. Please install a spaCy model.\")\n",
    "    print(\"Run 'pipenv run python3 -m spacy download pt_core_news_lg' to install the model.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "686d7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "  if text is None: return \"\"\n",
    "  text = re.sub(r'-\\n', '', text)\n",
    "  text = re.sub(r'\\[\\d+\\]', '', text)  # Remove [number] patterns\n",
    "  text = re.sub(r'\\(\\d+\\)', '', text)  # Remove (number) patterns\n",
    "  text = re.sub(r'\\([\\w\\s,.]+\\d{4}\\)', '', text)  # Remove (AUTOR et al., 2020) patterns\n",
    "  \n",
    "  lines = text.split('\\n')\n",
    "  cleaned_lines = []\n",
    "  for line in lines:\n",
    "    if re.search(r'Rev\\. Latino-Am\\. Enfermagem|www\\.eerp\\.usp\\.br/rlae|ISSN:|DOI:', line):\n",
    "      continue\n",
    "    if re.fullmatch(r'\\s*\\d+\\s*', line) or len(line.strip()) < 10:\n",
    "        continue\n",
    "    cleaned_lines.append(line.strip())\n",
    "\n",
    "  text = \" \".join(cleaned_lines)\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  text = re.sub(r'[ \\t]+', ' ', text)\n",
    "  return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bfc78a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sent_tfidf(sent_doc):\n",
    "  clear_tokens = [\n",
    "    token.lemma_.lower()\n",
    "    for token in sent_doc\n",
    "      if not token.is_stop \n",
    "         and not token.is_punct \n",
    "         and not token.is_space\n",
    "         and len(token.lemma_) > 1\n",
    "  ]\n",
    "  return \" \".join(clear_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "190dccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycld2 as cld2\n",
    "def detect_language(text, default_lang='pt'):\n",
    "  try:\n",
    "    isReliable, textBytesFound, details = cld2.detect(text)\n",
    "    \n",
    "    return details[0][1]\n",
    "  except Exception:\n",
    "    return default_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d60a35",
   "metadata": {},
   "source": [
    "# 3.Cálculo e Extração de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e23f2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89d8d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(sentences):\n",
    "  if not sentences:\n",
    "    print(\"No sentences provided for TF-IDF computation.\")\n",
    "    return None, None, []\n",
    "  \n",
    "  tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, # Ignorar termos que aparecem em menos de 2 sentenças\n",
    "    # max_df=0.95, # Ignorar termos que aparecem em mais de 95% das sentenças\n",
    "    # ngram_range=(1, 1) # Incluir apenas unigrams (palavras únicas)\n",
    "  )\n",
    "  \n",
    "  # Aprende o vocabulário e o IDF, e transforma o corpus em matriz TF-IDF\n",
    "  tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "  \n",
    "  # O vocabulário aprendido (lista de termos em ordem)\n",
    "  feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "  \n",
    "  return tfidf_vectorizer, feature_names_tfidf, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "344514c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extração da Frequência Bruta Total (Global) ---\n",
    "def brute_frequency_global(sentences):\n",
    "  if not sentences:\n",
    "    print(\"No sentences provided for global frequency computation.\")\n",
    "    return pd.DataFrame(columns=['Term', 'Frequency'])\n",
    "  \n",
    "  print(\"INFO: Computing global frequency of terms...\")\n",
    "  all_lemmas_corpus = []\n",
    "  for sent in sentences:\n",
    "    sent_lemmas = [\n",
    "      token.lemma_.lower()\n",
    "      for token in sent\n",
    "        if not token.is_stop \n",
    "           and not token.is_punct \n",
    "           and not token.is_space\n",
    "           and len(token.lemma_) > 1\n",
    "    ]\n",
    "    all_lemmas_corpus.extend(sent_lemmas)\n",
    "  \n",
    "  total_lemmas_count = Counter(all_lemmas_corpus)\n",
    "  terms_ordered_by_frequency_brute = total_lemmas_count.most_common()\n",
    "  df_brute_frequency = pd.DataFrame(terms_ordered_by_frequency_brute, columns=['Term', 'Frequency'])\n",
    "  \n",
    "  print(f\"INFO: Brute frequency computed for {len(total_lemmas_count)} terms.\")\n",
    "  print(\"INFO: Top 20 terms by frequency:\", terms_ordered_by_frequency_brute[:20])\n",
    "  return df_brute_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97138d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extração da Importância Total TF-IDF (Global) ---\n",
    "def tfidf_importance_global(feature_names_tfidf, tfidf_matrix):\n",
    "  if tfidf_matrix is None and feature_names_tfidf is None and tfidf_matrix.shape[0] == 0:\n",
    "    print(\"No valid TF-IDF data provided for global importance computation.\")\n",
    "    return pd.DataFrame(columns=['Term', 'Importance'])\n",
    "\n",
    "  print(\"INFO: Computing global TF-IDF importance of terms...\")\n",
    "  \n",
    "  tfidf_sum = tfidf_matrix.sum(axis=0).tolist()[0]\n",
    "  terms_importance = {}\n",
    "  for i, term in enumerate(feature_names_tfidf):\n",
    "    terms_importance[term] = tfidf_sum[i]\n",
    "  \n",
    "  terms_importance_ordered = sorted(terms_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "  df_tfidf_importance = pd.DataFrame(terms_importance_ordered, columns=['Term', 'Importance'])\n",
    "  \n",
    "  print(f\"INFO: Global TF-IDF importance computed for {len(terms_importance)} terms.\")\n",
    "  print(\"INFO: Top 20 terms by TF-IDF importance:\", terms_importance_ordered[:20])\n",
    "  return df_tfidf_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "868541de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preparar Dados para Planilha de Termos ---\n",
    "# Criar um DataFrame para consolidar as informações\n",
    "def prepare_terms_dataframe(terms_brute_df, terms_importance_df):\n",
    "  if terms_brute_df.empty or terms_importance_df.empty:\n",
    "    print(\"No terms provided for DataFrame preparation.\")\n",
    "    return pd.DataFrame()\n",
    "  \n",
    "  print(\"INFO: Preparing DataFrame for terms...\")\n",
    "  \n",
    "  df_terms = pd.merge(terms_brute_df, terms_importance_df, on='Term', how='outer').fillna(0)\n",
    "  # Remover termos com frequência ou importancia zero\n",
    "  df_terms = df_terms[df_terms['Frequency'] > 0]\n",
    "  df_terms = df_terms[df_terms['Importance'] > 0]\n",
    "  \n",
    "  # Remover termos com apenas números\n",
    "  df_terms = df_terms[~df_terms['Term'].str.match(r'^\\d+$')]\n",
    "\n",
    "  return df_terms\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a7ba7",
   "metadata": {},
   "source": [
    "# 4.Extração de Relações de Causa e Efeito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f25a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dicionários de Verbos Causais ---\n",
    "# TODO: substituir por leitura de arquivo externo\n",
    "CAUSAL_VERBS_LEMAS = {\n",
    "  \"causar\", \"provocar\", \"ocasionar\", \"gerar\", \"levar\", \"resultar\",\n",
    "  \"desencadear\", \"induzir\", \"promover\", \"acarretar\", \"implicar\", \n",
    "  \"produzir\", \"motivar\", \"suscitar\", \"originar\", \"contribuir\",\n",
    "  \"aumentar\", \"reduzir\", \"elevar\", \"diminuir\"\n",
    "}\n",
    "\n",
    "CAUSAL_NOUNS_LEMAS = {\n",
    "  \"fator\", \"motivo\", \"razão\", \"causa\", \"preditor\", \"marcador\", \"risco\", \n",
    "}\n",
    "\n",
    "EFFECT_NOUNS_LEMAS = {\n",
    "  \"consequencia\", \"resultado\", \"efeito\", \"impacto\", \"repercussão\", \"desfecho\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b30963d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_action_words(text):\n",
    "  doc = nlp(text)\n",
    "  filtered_tokens = []\n",
    "  ACTION_LEMMAS = { \"aumentar\", \"reduzir\", \"elevar\", \"elevação\", \"diminuir\", \"queda\"\n",
    "                   \"crescer\", \"crescimento\", \"diminuição\", \"redução\" }\n",
    "  for token in doc:\n",
    "    if token.lemma_.lower() not in ACTION_LEMMAS:\n",
    "      filtered_tokens.append(token.text_with_ws)\n",
    "  return \"\".join(filtered_tokens).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6cc3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_entity(entity, nlp_doc):\n",
    "  if len(entity) < 3:\n",
    "    return False\n",
    "  if re.search(r'\\d{4}|[\\[\\(]\\d+[\\]\\)]|http|www|ISSN|DOI', entity):\n",
    "    return False\n",
    "  \n",
    "  doc = nlp(entity)\n",
    "  \n",
    "  non_stop_tokens = [token for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "  if not non_stop_tokens:\n",
    "    return False\n",
    "  \n",
    "  has_content_word = any(token.pos_ in ['NOUN', 'PROPN', 'ADJ'] for token in doc)\n",
    "  if not has_content_word:\n",
    "    return False\n",
    "  \n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "829e177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_causal_relations(sent_doc):\n",
    "  if sent_doc is None: return []\n",
    "  \n",
    "  relations = []\n",
    "  \n",
    "  def get_entity_phrase(token):\n",
    "    tokens_in_phrase = []\n",
    "    for t in token.subtree:\n",
    "      if t.dep_ in ('relcl', 'advcl') and t != token:\n",
    "        break\n",
    "      tokens_in_phrase.append(t.text)\n",
    "    return \" \".join(tokens_in_phrase)\n",
    "  \n",
    "  for token in sent_doc:\n",
    "    # --- Lógica 1: Verbos Causais (Causa -> Verbo -> Efeito) \n",
    "    if token.lemma_ in CAUSAL_VERBS_LEMAS and token.pos_ == \"VERB\":\n",
    "      subjects = [child for child in token.children if child.dep_ in (\"nsubj\", \"nsubj:pass\")]\n",
    "      objects = [child for child in token.children if child.dep_ in (\"obj\", \"dobj\")]\n",
    "      obl_objects = [grandchild for child in token.children if child.dep_ == \"obl\" for grandchild in child.children if grandchild.dep_ == \"pobj\"]\n",
    "      objects.extend(obl_objects)\n",
    "      \n",
    "      for subj in subjects:\n",
    "        if len(list(subj.subtree)) > 1 or subj.pos_ not in ['PRON', 'DET']:\n",
    "          for obj in objects:\n",
    "            cause = get_entity_phrase(subj)\n",
    "            effect = get_entity_phrase(obj)\n",
    "            \n",
    "            if cause and effect:\n",
    "              cause = clear_action_words(cause)\n",
    "              effect = clear_action_words(effect)\n",
    "              if is_valid_entity(cause, sent_doc) and is_valid_entity(effect, sent_doc):\n",
    "                relations.append({\n",
    "                  \"tipo_marcador\": \"verbo\", \"marcador\": token.text,\n",
    "                  \"lema_marcador\": token.lemma_, \"causa\": cause, \"efeito\": effect,\n",
    "                  \"direcao\": \"Causa -> Efeito\", \"sentenca_original\": sent_doc.text\n",
    "                })\n",
    "    \n",
    "    # --- Lógica 2: Substantivos Causais (Causa -> Subst -> Efeito) ---\n",
    "    elif token.pos_ == \"NOUN\" and (token.lemma_ in CAUSAL_NOUNS_LEMAS or token.lemma_ in EFFECT_NOUNS_LEMAS):\n",
    "      is_cause_noun = token.lemma_ in CAUSAL_NOUNS_LEMAS\n",
    "      \n",
    "      main_entity_token = None\n",
    "      secondary_entity_token = None\n",
    "      \n",
    "      for child in token.children:\n",
    "        if child.dep_ == 'nmod':\n",
    "          secondary_entity_token = child\n",
    "          break\n",
    "      \n",
    "      if token.dep_ == 'attr':\n",
    "        main_entity_token = next((child for child in token.head.children if child.dep_ == 'nsubj'), None)\n",
    "      elif token.dep_ == 'nsubj':\n",
    "        main_entity_token = next((child for child in token.head.children if child.dep_ == 'attr'), None)\n",
    "      elif token.dep_ == 'ROOT':\n",
    "        main_entity_token = next((child for child in token.children if child.dep_ == 'appos'), None)\n",
    "        \n",
    "      if main_entity_token and secondary_entity_token:\n",
    "        main_entity_text = get_entity_phrase(main_entity_token)\n",
    "        secondary_entity_token = get_entity_phrase(secondary_entity_token)\n",
    "        \n",
    "        if is_cause_noun:\n",
    "          cause, effect = main_entity_text, secondary_entity_token\n",
    "          direction = \"Causa -> Efeito\"\n",
    "        else:\n",
    "          cause, effect = secondary_entity_token, main_entity_text\n",
    "          direction = \"Efeito <- Causa\"\n",
    "          \n",
    "        if cause and effect:\n",
    "          cause = clear_action_words(cause)\n",
    "          effect = clear_action_words(effect)\n",
    "          if is_valid_entity(cause, sent_doc) and is_valid_entity(effect, sent_doc):\n",
    "            relations.append({\n",
    "              \"tipo_marcador\": \"substantivo (\" + (\"causa\" if is_cause_noun else \"efeito\") + \")\",\n",
    "              \"marcador\": token.text, \"lema_marcador\": token.lemma_,\n",
    "              \"causa\": cause, \"efeito\": effect,\n",
    "              \"direcao\": direction, \"sentenca_original\": sent_doc.text\n",
    "            })\n",
    "  \n",
    "  unique_relations = []\n",
    "  seen = set()\n",
    "  for rel in relations:\n",
    "    identifier = (rel['causa'], rel['efeito'], rel['direcao'])\n",
    "    if identifier not in seen:\n",
    "      unique_relations.append(rel)\n",
    "      seen.add(identifier)\n",
    "  \n",
    "  return unique_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f95b7",
   "metadata": {},
   "source": [
    "# 5. Enriquecer as Relações com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73498745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_relations_with_tfidf(relations, tfidf_vectorizer):\n",
    "  if tfidf_vectorizer is None:\n",
    "    print(\"TF-IDF vectorizer is not provided. Skipping enrichment.\")\n",
    "    return relations\n",
    "  \n",
    "  vocabulary = {term: tfidf_vectorizer.idf_[idx] for term, idx in tfidf_vectorizer.vocabulary_.items()}\n",
    "  \n",
    "  enriched_relations = []\n",
    "  \n",
    "  for rel in relations:\n",
    "    new_relation = rel.copy()\n",
    "    \n",
    "    cause_doc = nlp(rel['causa'])\n",
    "    effect_doc = nlp(rel['efeito'])\n",
    "    \n",
    "    cause_terms = [t.lemma_.lower() for t in cause_doc if not t.is_stop and not t.is_punct]\n",
    "    effect_terms = [t.lemma_.lower() for t in effect_doc if not t.is_stop and not t.is_punct]\n",
    "    \n",
    "    score_cause_idf = []\n",
    "    score_effect_idf = []\n",
    "\n",
    "    for term in cause_terms:\n",
    "      if term in vocabulary:\n",
    "        score_cause_idf.append(vocabulary[term])\n",
    "    for term in effect_terms:\n",
    "      if term in vocabulary:\n",
    "        score_effect_idf.append(vocabulary[term])\n",
    "\n",
    "    score_cause = sum(score_cause_idf) / len(score_cause_idf) if score_cause_idf else 0\n",
    "    score_effect = sum(score_effect_idf) / len(score_effect_idf) if score_effect_idf else 0\n",
    "    \n",
    "    total_score = score_cause + score_effect\n",
    "    \n",
    "    new_relation['score_relacao'] = round(total_score, 2)\n",
    "    new_relation['termos_causa'] = \", \".join(cause_terms)\n",
    "    new_relation['termos_efeito'] = \", \".join(effect_terms)\n",
    "    enriched_relations.append(new_relation)\n",
    "  \n",
    "  enriched_relations.sort(key=lambda x: x['score_relacao'], reverse=True)\n",
    "  return enriched_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bc3f4",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646cb7f",
   "metadata": {},
   "source": [
    "## 1. Extração de Texto de PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0f848c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando arquivo: corpus\\ncomms5212.pdf\n",
      "  ... Texto extradio com sucesso (59025 caracteres)\n",
      "Processamento concluido. 1 arquivos processados.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# 1. Extrair texto bruto do PDF\n",
    "CORPUS_DIR = Path(\"corpus\")\n",
    "RECURSIVE = True\n",
    "\n",
    "TERMS_OUTPUT = \"terms_analysis.csv\"\n",
    "RELATIONS_OUTPUT = \"cause_effect_relations.csv\"\n",
    "\n",
    "brute_texts = []\n",
    "processed_files = []\n",
    "\n",
    "if RECURSIVE:\n",
    "  search_pattern = CORPUS_DIR.rglob(\"*.pdf\")\n",
    "else:\n",
    "  search_pattern = CORPUS_DIR.glob(\"*.pdf\")\n",
    "\n",
    "for pdf_path in search_pattern:\n",
    "  print(f\"Processando arquivo: {pdf_path}\")\n",
    "  extracted_text = extract_text_pymupdf(pdf_path)\n",
    "  \n",
    "  if extracted_text:\n",
    "    brute_texts.append(extracted_text)\n",
    "    processed_files.append(pdf_path.name)\n",
    "    print(f\"  ... Texto extradio com sucesso ({len(extracted_text)} caracteres)\")\n",
    "  else:\n",
    "    print(f\"Falha ao extrair texto de: {pdf_path}\")\n",
    "    continue\n",
    "\n",
    "print(f\"Processamento concluido. {len(brute_texts)} arquivos processados.\")\n",
    "\n",
    "if not brute_texts:\n",
    "  print(\"Nenhum texto foi extraído dos PDFs.\")\n",
    "  exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917b7c9",
   "metadata": {},
   "source": [
    "## 2.Pré-processamento do Texto Extraído"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39e26710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando pré-processamento de todo o corpus...\n",
      "Pré-processamento concluído. 9 sentenças extraídas do corpus.\n",
      "Total de sentenças ignoradas (não em português): 1006\n"
     ]
    }
   ],
   "source": [
    "# 2. Limpeza Inicial do Texto Bruto\n",
    "print(\"\\nIniciando pré-processamento de todo o corpus...\")\n",
    "sentences_docs = []\n",
    "info_sentences = []\n",
    "total_ignored = 0\n",
    "\n",
    "for i, brute_text in enumerate(brute_texts):\n",
    "  text_name = processed_files[i]\n",
    "  cleared_text = clear_text(brute_text)\n",
    "  doc_text = nlp(cleared_text)\n",
    "  for sent in doc_text.sents:\n",
    "    detected_lang = detect_language(sent.text)\n",
    "    if detected_lang != 'pt':\n",
    "      total_ignored += 1\n",
    "      continue\n",
    "    sentences_docs.append(sent)\n",
    "    info_sentences.append({\"arquivo_origem\": text_name})\n",
    "\n",
    "print(f\"Pré-processamento concluído. {len(sentences_docs)} sentenças extraídas do corpus.\")\n",
    "print(f\"Total de sentenças ignoradas (não em português): {total_ignored}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34eaf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preparar lista de sentenças limpas para TF-IDF\n",
    "sentences_tfidf = []\n",
    "for sent_doc in sentences_docs:\n",
    "    sentences_tfidf.append(preprocess_sent_tfidf(sent_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99525764",
   "metadata": {},
   "source": [
    "### **VISUALIZAÇÃO DAS SENTENÇAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feb8ff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentença 1: The Pearson correlation coefﬁcient (PCC) between the ratio of shared disease links and disease similarity is very high (PCC ¼ 0.96, P ¼ 1.4 \u0002 10 \u0003 5), indicating that the proposed disease similarity is a reliable measure for shared symptoms.\n",
      "Sentença 2: The number of overlapping links is signiﬁcantly (P ¼ 2.2 \u0002 10 \u0003 16, Supplementary Methods) than random expectation, again indicating that the HSDN offers reliable relationships.\n",
      "Sentença 3: 8-fold increase compared with random expectation, P ¼ 2.2 \u0002 10 \u0003 16, binomial test;\n",
      "Sentença 4: shows strong positive correlation with disease similarity (PCC ¼ 0.92 and P ¼ 1.8 \u0002 10 \u0003 4;\n",
      "Sentença 5: The ratio of diseases with shared PPIs increases signiﬁcantly with higher symptom similarity (PCC ¼ 0.89, P ¼ 5.4 \u0002 10 \u0003 4 for 1st order interactions, Fig.\n",
      "Sentença 6: Indeed, we ﬁnd strong negative correlation between the MSPL and symptom similarities (PCC ¼ \u0003 0.93 and P ¼ 7.7 \u0002 10 \u0003 5;\n",
      "Sentença 7: is statistically highly signiﬁcant (P ¼ 2.2 \u0002 10 \u0003 16, binomial test).\n",
      "Sentença 8: , ﬁnding strong positive correlations correlation: PCC ¼ 0.84, P ¼ 2.5 \u0002 10 \u0003 10, Fig.\n",
      "Sentença 9: 7a; betweenness correlation: PCC ¼ 0.59, P ¼ 9.5 \u0002 10 \u0003 7, Fig.\n"
     ]
    }
   ],
   "source": [
    "for i, sent_doc in enumerate(sentences_docs):\n",
    "    print(f\"Sentença {i+1}: {sent_doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eaea90f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentenças Limpas para TF-IDF ---\n",
      "Sentença 1 (limpa): the pearson correlation coefﬁcient pcc between the ratio of shared disease link and disease similarity is very high pcc 0.96 1.4 10 indicating that the proposed disease similarity is reliable measure shared symptoms\n",
      "Sentença 2 (limpa): the number of overlapping link is signiﬁcantly 2.2 10 16 supplementary methods than random expectation again indicating that the hsdn offers reliable relationships\n",
      "Sentença 3 (limpa): 8-fold increase compared with random expectation 2.2 10 16 binomial test\n",
      "Sentença 4 (limpa): show strong positive correlation with disease similarity pcc 0.92 and 1.8 10\n",
      "Sentença 5 (limpa): the ratio of diseases with shared ppis increases signiﬁcantly with higher symptom similarity pcc 0.89 5.4 10 1st order interactions fig\n",
      "Sentença 6 (limpa): indeed we ﬁnd strong negative correlation between the mspl and symptom similarities pcc 0.93 and 7.7 10\n",
      "Sentença 7 (limpa): is statistically highly signiﬁcant 2.2 10 16 binomial test\n",
      "Sentença 8 (limpa): ﬁnding strong positive correlations correlation pcc 0.84 2.5 10 10 fig\n",
      "Sentença 9 (limpa): 7a betweenness correlation pcc 0.59 9.5 10 fig\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sentenças Limpas para TF-IDF ---\")\n",
    "for i, sent_limpa in enumerate(sentences_tfidf):\n",
    "    print(f\"Sentença {i+1} (limpa): {sent_limpa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba1998",
   "metadata": {},
   "source": [
    "## 3.Cálculo e Extração de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "972d5609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF calculado. Corpus com 9 sentenças e 27 termos únicos.\n",
      "\n",
      "Scores TF-IDF para a primeira sentença:\n",
      "[('disease', np.float64(0.5140215757590505)), ('the', np.float64(0.39488426537531346)), ('shared', np.float64(0.34268105050603365)), ('is', np.float64(0.2979512813337321)), ('similarity', np.float64(0.2979512813337321)), ('pcc', np.float64(0.21094035011304343)), ('between', np.float64(0.17134052525301682)), ('indicating', np.float64(0.17134052525301682)), ('link', np.float64(0.17134052525301682)), ('ratio', np.float64(0.17134052525301682))]\n"
     ]
    }
   ],
   "source": [
    "# 4. Calcular TF-IDF\n",
    "tfidf_vectorizer, feature_names_tfidf, tfidf_matrix = compute_tfidf(sentences_tfidf)\n",
    "if tfidf_vectorizer is not None:\n",
    "    print(f\"TF-IDF calculado. Corpus com {tfidf_matrix.shape[0]} sentenças e {tfidf_matrix.shape[1]} termos únicos.\")\n",
    "    print(\"\\nScores TF-IDF para a primeira sentença:\")\n",
    "    primeira_sentenca_tfidf_scores = tfidf_matrix[0].T.todense() # Converte para matriz densa\n",
    "    termos_scores = [(feature_names_tfidf[i], primeira_sentenca_tfidf_scores[i, 0]) for i in range(len(feature_names_tfidf)) if primeira_sentenca_tfidf_scores[i, 0] > 0]\n",
    "    print(sorted(termos_scores, key=lambda item: item[1], reverse=True)[:10]) # Top 10 scores na primeira sentença\n",
    "else:\n",
    "    print(\"Erro ao computar TF-IDF. Verifique as sentenças fornecidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28481875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Computing global frequency of terms...\n",
      "INFO: Brute frequency computed for 80 terms.\n",
      "INFO: Top 20 terms by frequency: [('10', 10), ('the', 7), ('pcc', 7), ('correlation', 5), ('disease', 4), ('and', 4), ('similarity', 4), ('is', 4), ('with', 4), ('of', 3), ('shared', 3), ('2.2', 3), ('16', 3), ('strong', 3), ('fig', 3), ('between', 2), ('ratio', 2), ('link', 2), ('indicating', 2), ('that', 2)]\n",
      "INFO: Computing global TF-IDF importance of terms...\n",
      "INFO: Global TF-IDF importance computed for 27 terms.\n",
      "INFO: Top 20 terms by TF-IDF importance: [('10', 1.9063008143594562), ('pcc', 1.6351667229209863), ('correlation', 1.4916212618258036), ('the', 1.360898862898396), ('fig', 1.3399586075048224), ('with', 1.2537699746614224), ('and', 1.1398140531508894), ('strong', 1.0968038874933725), ('16', 1.0672884470561192), ('is', 1.0016265256868295), ('binomial', 0.936827742256019), ('test', 0.936827742256019), ('disease', 0.919621430575187), ('similarity', 0.9193583500235566), ('positive', 0.8944671469848855), ('expectation', 0.7088872159590831), ('random', 0.7088872159590831), ('symptom', 0.6760894114896716), ('of', 0.6704691122531801), ('shared', 0.6517766524011144)]\n",
      "INFO: Preparing DataFrame for terms...\n",
      "INFO: DataFrame prepared and saved to 'terms_analysis.csv'.\n"
     ]
    }
   ],
   "source": [
    "# 5. Extração da Frequência Bruta Total (Global)\n",
    "terms_brute_df = brute_frequency_global(sentences_docs)\n",
    "# 6. Extração da Importância Total TF-IDF (Global)\n",
    "terms_importance_df = tfidf_importance_global(feature_names_tfidf, tfidf_matrix)\n",
    "# 7. Criar DataFrame com os resultados\n",
    "df_terms = prepare_terms_dataframe(terms_brute_df, terms_importance_df)\n",
    "\n",
    "df_terms.to_csv('terms_analysis.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"INFO: DataFrame prepared and saved to 'terms_analysis.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ccfa2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 20 Terms by Importance ---\n",
      "           Term  Frequency  Importance\n",
      "51          pcc          7    1.635167\n",
      "25  correlation          5    1.491621\n",
      "74          the          7    1.360899\n",
      "30          fig          3    1.339959\n",
      "77         with          4    1.253770\n",
      "19          and          4    1.139814\n",
      "67       strong          3    1.096804\n",
      "40           is          4    1.001627\n",
      "22     binomial          2    0.936828\n",
      "71         test          2    0.936828\n"
     ]
    }
   ],
   "source": [
    "# Terms ordered by frequency\n",
    "print(\"\\n--- Top 20 Terms by Importance ---\")\n",
    "print(df_terms.sort_values(by='Importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e8073",
   "metadata": {},
   "source": [
    "## 4.Extração de Relações de Causa e Efeito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3602153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de sentenças processadas: 9\n",
      "\n",
      "--- Resumo das Relações Causais Encontradas ---\n",
      "Nenhuma relação de causa e efeito encontrada no corpus.\n"
     ]
    }
   ],
   "source": [
    "# 8. Extração de Relações Causais\n",
    "all_relations_found = []\n",
    "for i, sent_doc in enumerate(sentences_docs):\n",
    "  sentence_relations = extract_causal_relations(sentences_docs[i])\n",
    "  if sentence_relations:\n",
    "    for rel in sentence_relations:\n",
    "      rel['arquivo_origem'] = info_sentences[i]['arquivo_origem']\n",
    "    all_relations_found.extend(sentence_relations)\n",
    "    print(f\"Encontradas {len(sentence_relations)} relações causais na sentença {i+1}: \\\"{sent_doc.text}\\\".\")\n",
    "\n",
    "print(f\"Total de sentenças processadas: {len(sentences_docs)}\")\n",
    "# 9. Exibir as relações causais encontradas\n",
    "print(\"\\n--- Resumo das Relações Causais Encontradas ---\")\n",
    "if all_relations_found:\n",
    "  df_relations = pd.DataFrame(all_relations_found)\n",
    "  if not df_relations.empty:\n",
    "    ordered_columns = ['tipo_marcador', 'lema_marcador', 'causa', 'marcador', 'efeito', 'direcao', 'sentenca_original', 'arquivo_origem']\n",
    "    df_relations = df_relations[ordered_columns]\n",
    "    df_relations.to_csv(\"cause_effect_relations.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(\"Arquivo 'cause_effect_relations.csv' gerado.\")\n",
    "\n",
    "    print(\"\\nPrimeiras 10 relações causais encontradas:\")\n",
    "    print(df_relations.head(10))\n",
    "  else:\n",
    "    print(\"Nenhuma relação de causa e efeito única encontrada para gerar o CSV.\")\n",
    "else:\n",
    "  print(\"Nenhuma relação de causa e efeito encontrada no corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6450f",
   "metadata": {},
   "source": [
    "## 5. Enriquecer as Relações com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6f12283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Enriquecendo Relações com Scores TF-IDF ---\n",
      "Nenhuma relação causal encontrada para enriquecer com TF-IDF.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Enriquecendo Relações com Scores TF-IDF ---\")\n",
    "\n",
    "enriched_relations = enrich_relations_with_tfidf(all_relations_found, tfidf_vectorizer)\n",
    "\n",
    "if enriched_relations:\n",
    "    df_enriched_relations = pd.DataFrame(enriched_relations)\n",
    "    if not df_enriched_relations.empty:\n",
    "        ordered_columns = ['score_relacao', 'direcao', 'tipo_marcador',\n",
    "            'causa', 'marcador', 'efeito', 'termos_causa', 'termos_efeito', 'sentenca_original','arquivo_origem']\n",
    "        final_columns = [col for col in ordered_columns if col in df_enriched_relations.columns]\n",
    "        df_enriched_relations = df_enriched_relations[final_columns]\n",
    "        \n",
    "        df_enriched_relations.to_csv(\"enriched_cause_effect_relations.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(\"Arquivo 'enriched_cause_effect_relations.csv' gerado com sucesso.\")\n",
    "        print(\"\\nPrimeiras 10 relações enriquecidas:\")\n",
    "        print(df_enriched_relations[['score_relacao', 'causa', 'efeito', 'direcao']].head(10))\n",
    "    else:\n",
    "        print(\"Nenhuma relação enriquecida encontrada para gerar o CSV.\")\n",
    "else:\n",
    "    print(\"Nenhuma relação causal encontrada para enriquecer com TF-IDF.\")\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
